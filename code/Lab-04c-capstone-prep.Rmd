---
title: "Lab-04c-capstone-prep"
author: "Peyton Politewicz"
date: "2024-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Capstone Prep

## Introduction

A lot of the mechanics behind data science work is messy, but today we're going to do our best to get started on the right foot and sidestep some of that! This document is going to help you get situated when you start working on your capstone project. There are many approaches to an open-ended problem like a research project, and this is just one of them; as you learn what you like and don't like about R and R markdown, about how to structure your work, feel free to change things. Burn it all down and start again, if you choose!

This is just one example of what good might look like.

## Research Question(s)

**Always** start with this - you need to know where your work is headed to help guide all of your analysis.

In the case of this project, and quite often in research, you're going to be given a dataset and you might not have an explicit objective given to you.

Start with an open mind - recall exploratory data analysis. You've got a totally blank slate, so your first task might be to find out what's interesting about a dataset that begets further investigation.

## Initialization

We're going to start building our toolbox by adding our functions. Whenever we want to use a new tool, we're going to come back up here and add it into this code block.

```{r initialization}
#
# The libraries you're going to use should go here, at the very top.
# Add tidyverse. You know you're gonna use tidyverse!
# Consider a unique random seed: set.seed(ANumberThatYouHappenToLike)
#

library(tidyverse)
library(rsample)
library(glmnet)
library(grpreg)
library(grplasso)
library(sparsegl)

set.seed(0)
```

## Data Import

Typically, I split this section into two pieces; one explicitly for imports and merging,     and another explicitly for data cleaning operations.

**An important note** is filtering and trimming your dataset down is different than cleaning; this first section is about getting your data into a 'minimum usable state', renaming things, removing records that are erroneous or full of 'NA' or 'NULL' values, and so forth. I, personally, wouldn't start filtering, subetting, or other such tasks until I was writing code that dealt with those subsets.

**Another important note** is that you should take notes in comments (lines beginning with #) to keep track of what occurs here - it's typically requested when you author research papers to note how many observations were removed, and for what reasons.

```{r data_import}
#
#read_csv()...
#data <- left_join(data, otherdata, by = matchingFeature)
#

parkinsons_dataset <- read_csv("../data/dataset.csv")
```

```{r data_cleaning}
# turn all - into NA DONE
parkinsons_dataset <- parkinsons_dataset %>% 
  mutate(
    across(
      everything(),
      ~ case_when(
        . == "-" ~ NA,
        TRUE ~ .
      )
    )
  )

# turn yes and no into logicals DONE
parkinsons_dataset <- parkinsons_dataset %>% 
  mutate(
    across(4,
      ~ case_when(
        . == "Yes" ~ TRUE,
        . == "No" ~ FALSE
      )
    )
  )

# turn yes and no for medication into medication name or none DONE
parkinsons_dataset <- parkinsons_dataset %>% 
  mutate(
    across(7:10,
      ~ case_when(
        substr(., 1, 3) == "Yes" ~ substr(., 7, nchar(.) - 1),
        . == "No" ~ "None"
      )
    )
  )

# turn columns into factors
parkinsons_dataset$Gender <- parkinsons_dataset$Gender %>%
  as.factor()

parkinsons_dataset[[13]] <- parkinsons_dataset[[13]] %>%
  factor(levels = c("1", "1.5", "2", "2.5", "3", "3.5", "4", "4.5", "5"))

parkinsons_dataset[, 15:41] <- parkinsons_dataset[, 15:41] %>% 
  lapply(factor, levels = 0:4)

parkinsons_dataset[, c(5, 6, 14)] <- parkinsons_dataset[, c(5, 6, 14)] %>% 
  lapply(as.numeric)

# rename all variables
parkinsons_dataset <- parkinsons_dataset %>% 
  rename(id = `Participant  code`, age = `Age  (years)`, gender = Gender, fam_history = `Positive  history  of  Parkinson  disease  in  family`, age_onset = `Age  of  disease  onset  (years)`, duration = `Duration  of  disease  from  first  symptoms  (years)`)

parkinsons_dataset <- parkinsons_dataset %>% 
  rename(antidep_meds = `Antidepressant  therapy`, antipark_meds = `Antiparkinsonian  medication`, antipsych_meds = `Antipsychotic  medication`, benz_meds = `Benzodiazepine  medication`, ldopa = `Levodopa  equivalent  (mg/day)`, clonaz = `Clonazepam  (mg/day)`, hoen_yahr_motor = `Overview  of  motor  examination:  Hoehn  &  Yahr  scale  (-)`, updrs = `Overview  of  motor  examination:  UPDRS  III  total  (-)`, speech = `18.  Speech`, facial = `19.  Facial  Expression`, tremorRest_head = `20.  Tremor  at  Rest  -  head`, tremorRest_RUE = `20.  Tremor  at  Rest  -  RUE`, tremorRest_LUE = `20.  Tremor  at  Rest  -  LUE`, tremorRest_RLE = `20.  Tremor  at  Rest  -  RLE`, tremorRest_LLE = `20.  Tremor  at  Rest  -  LLE`, AP_tremor_RUE = `21.  Action  or  Postural  Tremor  -  RUE`, AP_tremor_LUE = `21.  Action  or  Postural  Tremor  -  LUE`, rigid_neck = `22.  Rigidity  -  neck`, rigid_RUE = `22.  Rigidity  -  RUE`, rigid_LUE = `22.  Rigidity  -  LUE`, rigid_RLE = `22.  Rigidity  -  RLE`, rigid_LLE = `22.  Rigidity  -  LLE`)

parkinsons_dataset <- parkinsons_dataset %>% 
  rename(fingertaps_RUE = `23.Finger  Taps  -  RUE`, fingertaps_LUE = `23.Finger  Taps  -  LUE`, handmove_RUE = `24.  Hand  Movements  -  RUE`, handmove_LUE = `24.  Hand  Movements  -  LUE`, rapidAltMove_RUE = `25.  Rapid  Alternating  Movements  -  RUE`, rapidAltMove_LUE = `25.  Rapid  Alternating  Movements  -  LUE`, legAg_RLE = `26.  Leg  Agility  -  RLE`, legAG_LLE = `26.  Leg  Agility  -  LLE`, chair = `27.  Arising  from  Chair`, posture = `28.  Posture`, gait = `29.  Gait`, post_stability = `30.  Postural  Stability`, brady_hypo = `31.  Body  Bradykinesia  and  Hypokinesia`)

parkinsons_dataset <- parkinsons_dataset %>% 
  rename(entropy1 = `Entropy  of  speech  timing  (-)...42`, rate_speech_time1 = `Rate  of  speech  timing  (-/min)...43`, accel_speech_time1 = `Acceleration  of  speech  timing  (-/min2)...44`, dur_pause_int1 = `Duration  of  pause  intervals  (ms)...45`, dur_voice_int1 = `Duration  of  voiced  intervals  (ms)...46`, gap_voiced_int1 = `Gaping  in-between  voiced  intervals  (-/min)`, dur_unvoiced_stops1 = `Duration  of  unvoiced  stops  (ms)...48`, decay_fric1 = `Decay  of  unvoiced  fricatives  (‰/min)...49`, rel_loud_resp1 = `Relative  loudness  of  respiration  (dB)...50`, pause_int_resp1 = `Pause  intervals  per  respiration  (-)...51`, rate_speech_resp1 = `Rate  of  speech  respiration  (-/min)...52`, latency_resp1 = `Latency  of  respiratory  exchange  (ms)...53`)

parkinsons_dataset <- parkinsons_dataset %>% 
  rename(entropy2 = `Entropy  of  speech  timing  (-)...54`, rate_speech_time2 = `Rate  of  speech  timing  (-/min)...55`, accel_speech_time2 = `Acceleration  of  speech  timing  (-/min2)...56`, dur_pause_int2 = `Duration  of  pause  intervals  (ms)...57`, dur_voice_int2 = `Duration  of  voiced  intervals  (ms)...58`, gap_voiced_int2 = `Gaping  in-between  voiced  Intervals  (-/min)`, dur_unvoiced_stops2 = `Duration  of  unvoiced  stops  (ms)...60`, decay_fric2 = `Decay  of  unvoiced  fricatives  (‰/min)...61`, rel_loud_resp2 = `Relative  loudness  of  respiration  (dB)...62`, pause_int_resp2 = `Pause  intervals  per  respiration  (-)...63`, rate_speech_resp2 = `Rate  of  speech  respiration  (-/min)...64`, latency_resp2 = `Latency  of  respiratory  exchange  (ms)...65`)

# turn participate code into new ID number
# create diagnosis column (HC = healthy, RBD = REM sleep disorder, PD = PD)
parkinsons_dataset <- parkinsons_dataset %>% 
  mutate(
    diagnosis = substr(id, 1, nchar(id) - 2),
    id = row_number()
  )

parkinsons_dataset$diagnosis <- parkinsons_dataset$diagnosis %>% 
  as.factor()

parkinsons_dataset <- parkinsons_dataset %>% 
  dplyr::select(id, diagnosis, everything())

# new data wrangling stuff
parkinsons_dataset <- parkinsons_dataset %>% 
  mutate(
    pd = case_when(
      diagnosis == "PD" ~ TRUE,
      TRUE ~ FALSE
    ),
    rbd = case_when(
      diagnosis == "RBD" ~ TRUE,
      TRUE ~ FALSE
    )
  ) %>% 
  dplyr::select(id, diagnosis, pd, rbd, everything())

```

## Models, Statistical Tests and Fusion

Once your data is in a workable state, here is where you'll run regressions and comparative tests to generate results to test the statistical questions you landed on after performing exploratory data analysis.

Operate with the same diligence as before - separate chunks for separate tasks. Annotate everything.

```{r}
# rbd version, no pd
parkinsons_dataset <- parkinsons_dataset[31:130,]

data_split <- initial_split(parkinsons_dataset, prop = 0.7)

train_data <- training(data_split)
test_data <- testing(data_split)

# create 2 models (read/1 variables verse monologue/2 variables)
logistic_model_voice1 <- glm(rbd ~ entropy1 + rate_speech_time1 + accel_speech_time1 + dur_pause_int1 + dur_voice_int1 + gap_voiced_int1 + dur_unvoiced_stops1 + decay_fric1 + rel_loud_resp1 + pause_int_resp1 + rate_speech_resp1 + latency_resp1, family = binomial, data = train_data)
summary(logistic_model_voice1)

logistic_model_voice2 <- glm(rbd ~ entropy2 + rate_speech_time2 + accel_speech_time2 + dur_pause_int2 + dur_voice_int2 + gap_voiced_int2 + dur_unvoiced_stops2 + decay_fric2 + rel_loud_resp2 + pause_int_resp2 + rate_speech_resp2 + latency_resp2, family = binomial, data = train_data)
summary(logistic_model_voice2)

# train the models with test data separately and test the models separately
pred_prob1 <- logistic_model_voice1 %>% 
  predict(test_data, type = "response")

pred_prob2 <- logistic_model_voice2 %>% 
  predict(test_data, type = "response")

pred_prob_mean <- (pred_prob1 + pred_prob2) / 2

# average the output
predicted_classes1 <- ifelse(pred_prob1 > 0.5, TRUE, FALSE)
predicted_classes2 <- ifelse(pred_prob2 > 0.5, TRUE, FALSE)
predicted_classes_mean <- ifelse(pred_prob_mean > 0.5, TRUE, FALSE)

# find the accuracy
mean(predicted_classes1 == test_data$rbd)
mean(predicted_classes2 == test_data$rbd)
mean(predicted_classes_mean == test_data$rbd)

# lasso
x <- parkinsons_dataset %>% 
  dplyr::select(entropy1, rate_speech_time1, accel_speech_time1, dur_pause_int1, dur_voice_int1, gap_voiced_int1, dur_unvoiced_stops1, decay_fric1, rel_loud_resp1, pause_int_resp1, rate_speech_resp1, latency_resp1, entropy2, rate_speech_time2, accel_speech_time2, dur_pause_int2, dur_voice_int2, gap_voiced_int2, dur_unvoiced_stops2, decay_fric2, rel_loud_resp2, pause_int_resp2, rate_speech_resp2, latency_resp2) %>% 
  as.matrix()

lasso_model <- cv.glmnet(x, parkinsons_dataset$rbd, family = "binomial")

plot(lasso_model)

coef(lasso_model, s = lasso_model$lambda.min)

lambda_values <- log(lasso_model$lambda)
cv_errors <- lasso_model$cvm

points(lambda_values, cv_errors, col = "deepskyblue", pch = 16)

# refined model
logistic_model_refined <- glm(rbd ~ rate_speech_time1 + accel_speech_time1 + dur_pause_int1 + gap_voiced_int1 + decay_fric1 + rel_loud_resp1 + rate_speech_resp1 + latency_resp1 + rate_speech_time2 + dur_unvoiced_stops2 + decay_fric2 + rate_speech_resp2, family = binomial, data = train_data)
summary(logistic_model_refined)

pred_prob_refined <- logistic_model_refined %>% 
  predict(test_data, type = "response")

predicted_classes_refined <- ifelse(pred_prob1 > 0.5, TRUE, FALSE)

mean(predicted_classes_refined == test_data$rbd)
```

```{r}
# split the data (train - 70 to test - 30)
data_split <- initial_split(parkinsons_dataset, prop = 0.7)

train_data <- training(data_split)
test_data <- testing(data_split)

# create 2 models (read/1 variables verse monologue/2 variables)
logistic_model_voice1 <- glm(pd ~ entropy1 + rate_speech_time1 + accel_speech_time1 + dur_pause_int1 + dur_voice_int1 + gap_voiced_int1 + dur_unvoiced_stops1 + decay_fric1 + rel_loud_resp1 + pause_int_resp1 + rate_speech_resp1 + latency_resp1, family = binomial, data = train_data)
summary(logistic_model_voice1)

logistic_model_voice2 <- glm(pd ~ entropy2 + rate_speech_time2 + accel_speech_time2 + dur_pause_int2 + dur_voice_int2 + gap_voiced_int2 + dur_unvoiced_stops2 + decay_fric2 + rel_loud_resp2 + pause_int_resp2 + rate_speech_resp2 + latency_resp2, family = binomial, data = train_data)
summary(logistic_model_voice2)

# train the models with test data separately and test the models separately
pred_prob1 <- logistic_model_voice1 %>% 
  predict(test_data, type = "response")

pred_prob2 <- logistic_model_voice2 %>% 
  predict(test_data, type = "response")

pred_prob_mean <- (pred_prob1 + pred_prob2) / 2

# average the output
predicted_classes1 <- ifelse(pred_prob1 > 0.5, TRUE, FALSE)
predicted_classes2 <- ifelse(pred_prob2 > 0.5, TRUE, FALSE)
predicted_classes_mean <- ifelse(pred_prob_mean > 0.5, TRUE, FALSE)

# find the accuracy
mean(predicted_classes1 == test_data$pd)
mean(predicted_classes2 == test_data$pd)
mean(predicted_classes_mean == test_data$pd)
```

```{r}
# lasso
x <- parkinsons_dataset %>% 
  dplyr::select(entropy1, rate_speech_time1, accel_speech_time1, dur_pause_int1, dur_voice_int1, gap_voiced_int1, dur_unvoiced_stops1, decay_fric1, rel_loud_resp1, pause_int_resp1, rate_speech_resp1, latency_resp1, entropy2, rate_speech_time2, accel_speech_time2, dur_pause_int2, dur_voice_int2, gap_voiced_int2, dur_unvoiced_stops2, decay_fric2, rel_loud_resp2, pause_int_resp2, rate_speech_resp2, latency_resp2) %>% 
  as.matrix()

lasso_model <- cv.glmnet(x, parkinsons_dataset$pd, family = "binomial")

plot(lasso_model)

coef(lasso_model, s = lasso_model$lambda.min)

lambda_values <- log(lasso_model$lambda)
cv_errors <- lasso_model$cvm

points(lambda_values, cv_errors, col = "deepskyblue", pch = 16)

# only important variables: rate_speech_time1, dur_pause_int1, dur_pause_int2
```

```{r}
# using grpreg (all 0s)
groups <- rep(1:2, each = 12)

lasso_model2 <- grpreg(x, parkinsons_dataset$pd, group = groups, penalty = "grLasso", family = "binomial", alpha = 0.5, standardize = TRUE)

plot(lasso_model2)

select(lasso_model2, "BIC")
```

```{r}
# using grplasso (still all 0s)
groups_with_intercept <- c(NA, groups)

x_with_intercept <- cbind(rep(1, 130), x)

lambda <- lambdamax(x_with_intercept, as.numeric(parkinsons_dataset$pd), index = groups_with_intercept, penscale = sqrt, model = LogReg()) * 0.5^(0:10)

lasso_model3 <- grplasso(x_with_intercept, as.numeric(parkinsons_dataset$pd), index = groups_with_intercept, lambda = lambda, model = LogReg(), standardize = TRUE)

plot(lasso_model3)

coef(lasso_model3)
```

```{r}
# plotting entropy 1 and 2 (no difference whatsoever)
parkinsons_dataset %>% 
  ggplot(aes(x = entropy1, y = pd)) +
  geom_jitter()

parkinsons_dataset %>% 
  ggplot(aes(x = entropy2, y = pd)) +
  geom_jitter()
```

```{r}
# using sparesgl (ALSO all 0s)
lasso_model4 <- cv.sparsegl(x, parkinsons_dataset$pd, group = groups, family = "binomial", pred.loss = "mse")

plot(lasso_model4)

coef(lasso_model4, s = "lambda.1se")
```

```{r}
# graphing important variables: rate_speech_time1, dur_pause_int1, dur_pause_int2
parkinsons_dataset %>% 
  ggplot(aes(x = rate_speech_time1, y = pd, color = pd)) +
  geom_jitter(size = 3) +
  scale_color_manual(values = c("deepskyblue", "orange"), labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  scale_y_discrete(labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  labs(x = "Rate of Speech Timing from Reading", y = "Status of Parkinson's Disease",  title = "Reading RST of Patients with PD Compared to those without PD", color = NULL)

parkinsons_dataset %>% 
  ggplot(aes(x = dur_pause_int1, y = pd, color = pd)) +
  geom_jitter(size = 3) +
  scale_color_manual(values = c("deepskyblue", "orange"), labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  scale_y_discrete(labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  labs(x = "Duration of Pause Intervals from Reading (ms)", y = "Status of Parkinson's Disease",  title = "Reading DPI of Patients with PD Compared to those without PD", color = NULL)

parkinsons_dataset %>% 
  ggplot(aes(x = dur_pause_int2, y = pd, color = pd)) +
  geom_jitter(size = 3) +
  scale_color_manual(values = c("deepskyblue", "cadetblue2"), labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  scale_y_discrete(labels = c("TRUE" = "PD", "FALSE" = "No PD")) +
  labs(x = "Duration of Pause Intervals from Monologuing (ms)", y = "Status of Parkinson's Disease",  title = "Monologuing DPI of Patients with PD Compared to those without PD", color = NULL)
```

```{r}
# dur_unvoiced_stops2
ggplot(parkinsons_dataset, aes(x = dur_unvoiced_stops2, y = rbd, fill = rbd)) + 
  geom_violin(alpha = 0.4) +
  geom_boxplot(alpha = 0.6) +
  stat_summary(fun.y = "mean", geom = "point", shape = 18, size = 3, color = "navyblue") +
  scale_fill_manual(values = c("deepskyblue","orange")) +
  scale_y_discrete(labels = c("TRUE" = "High Risk", "FALSE" = "Healthy")) +
  labs(x = "Duration of Unvoiced Stops from Monologuing (ms)", y = "Risk Status of PD",  title = "Monologuing DUS of High Risk Patients Compared Healthy Patients", fill = NULL) +
  theme_bw() +
  theme(legend.position = "none")

# decay_fric2
ggplot(parkinsons_dataset, aes(x = decay_fric2, y = rbd, fill = rbd)) + 
  geom_violin(alpha = 0.4) +
  geom_boxplot(alpha = 0.6) +
  stat_summary(fun.y = "mean", geom = "point", shape = 18, size = 3, color = "navyblue") +
  scale_fill_manual(values = c("deepskyblue","orange")) +
  scale_y_discrete(labels = c("TRUE" = "High Risk", "FALSE" = "Healthy")) +
  labs(x = "Decay of Unvoiced Fricative from Monologuing (‰/min)", y = "Risk Status of PD",  title = "Monologuing DUF of High Risk Patients Compared Healthy Patients", fill = NULL) +
  theme_bw() +
  theme(legend.position = "none")

# rate_speech_resp2
ggplot(parkinsons_dataset, aes(x = rate_speech_resp2, y = rbd, fill = rbd)) + 
  geom_violin(alpha = 0.4) +
  geom_boxplot(alpha = 0.6) +
  stat_summary(fun.y = "mean", geom = "point", shape = 18, size = 3, color = "navyblue") +
  scale_fill_manual(values = c("deepskyblue","orange"), labels = c("TRUE" = "High Risk", "FALSE" = "Healthy")) +
  scale_y_discrete(labels = c("TRUE" = "High Risk", "FALSE" = "Healthy")) +
  labs(x = "Rate of Speech Respiration from Monologuing (-/min)", y = "Risk Status of PD",  title = "Monologuing RSR of High Risk Patients Compared Healthy Patients", fill = NULL) +
  theme_bw() +
  theme(legend.position = "none")
```


```{r}
# test and train important variables
logistic_model_important <- glm(pd ~ rate_speech_time1 + dur_pause_int1 + dur_pause_int2, family = binomial, data = train_data)
summary(logistic_model_important)

pred_prob_important <- logistic_model_important %>% 
  predict(test_data, type = "response")

predicted_classes_important <- ifelse(pred_prob_important > 0.5, TRUE, FALSE)

mean(predicted_classes_important == test_data$pd)

# IGNORE THE P-VALUES
```

## Results

Your results can typically be summarized with a set of **four** values, recall:
An estimated value - what it says on the tin!
A p-value is a result's statistical significance -  (lower is better - the chance that your result would have occurred under pure random chance)
A confidence interval - (a range in which we're some percentage [typically 95 or 99%] sure the value can take)
An r-squared value - (how much of the data's total variance is explained by the model - you'll see this early next week!)

Save this section for extracting relevant results from the models and statistical test objects you generate above. Transform, present, and display them as necessary.

## Conclusions

This final part is almost all written; do your findings validate your research question? Are they significant? Are they significant *and* impactful? Are they significant, impactful, *and* precise? Two out of three?

Think about and talk about the implications of what you've discovered.